{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial 8b - Markov Chain Monte Carlo**\n",
    "\n",
    "In this tutorial, we will apply a Metropolis-Hastings Markov Chain (MCMC) sampler to type Ia supernova data.\n",
    "\n",
    "The Data will be from the Supernova Cosmology Project at:\n",
    "http://supernova.lbl.gov/union/descriptions.html#Magvsz\n",
    "\n",
    "Background:\n",
    "\n",
    "The apparent magnitude of an object with luminosity $L$ is\n",
    "\n",
    "$m = - 2.5 \\log\\left( \\frac{L}{2\\pi D_L^2} \\right) + m_o = 5 \\log\\left( D_L \\right) + 2.5 \\log\\left( L \\right) + m_o$\n",
    "\n",
    "where $D_L$ is the luminosity distance.  The peak luminosity of a type Ia supernova \n",
    "is directly related to the width of its lightcurve and its color.  In this data set, the correction to a standard candle has already been done and it is reported in terms of the estimated distance modulus\n",
    "\n",
    "$\\mu = 5 \\log\\left( D_L \\right) - 5$\n",
    "\n",
    "This assumes a Hubble constant and requires a calibration using other distance indicators in local galaxies so there is an additive constant to the distance modulus, or a multiplicative constant to the brightness, that is not very well constrained, i.e. the relative brightnesses of the supernovae are well measured, but not their absolute brightnesses.\n",
    "\n",
    "General relativity and energy conservation predict $D_L(z)$, or $\\mu(z)$, where $z$ is the cosmological redshift of the supernovae.  This relationship depends on the cosmological parameters, $\\Omega_{m}$ (the average density of matter) and $\\Omega_\\Lambda$ (the density of dark energy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Read in the supernova data from `SCPUnion2.1_mu_vs_z.txt` and plot distance modulus vs redshift with error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "data = pa.read_csv(\"SCPUnion2.1_mu_vs_z.txt\",sep='\\t',comment='#')\n",
    "\n",
    "z = ...\n",
    "mu = ...\n",
    "mu_err = ...\n",
    "\n",
    ".\n",
    ".\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Make a function that takes the parameters `p` and the redshift `z`.  It should return the predicted distance modulus.  Use `astropy.cosmology.cosmo.LambdaCDM()` to calculate the luminosity distance.  The parameters should be `p['M']`, `p['Omega_m']` and `p['Omega_Lambda']`.  It is not possible to measure the Hubble parameter independently from `M`, so just set it equal to 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.cosmology as cosmo\n",
    "def mu_model(p,z):\n",
    "    cosmo = \n",
    "    ...\n",
    "    return 5* ... + ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Make two functions. One is $\\chi^2$ using the data that has been uploaded and assuming the distance moduli are normally distributed.  Also, make `lnprob(p)` ,which returns the log of the Gaussian likelihood and uses the $\\chi^2$ function whithin it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(p) :\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "def lnprob(p) :\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We are going to use the library `lmfit`.  This library handles \n",
    "parameters through the class `lmfit.Parameters()`.  Here, create an instance of this class.  Each parameter has a name, an initial guess, whether it is varied, the minimum allowed value and the maximum allowed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit\n",
    "\n",
    "params = lmfit.Parameters()\n",
    "params.add_many(('Omega_m',0.2,True,0,1)\n",
    "           ,('M',...)\n",
    "           ,('Omega_Lambda',...)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now we can find the minimum of the chi-squared using `lmfit.minimize()`.\n",
    "\n",
    "Information on the output of lmfit.minimize() can be found at https://lmfit.github.io/lmfit-py/fitting.html under \"MinimizerResult â€“ the optimization result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = lmfit.minimize(chi_squared, params, method='Nelder')\n",
    "lmfit.printfuncs.report_fit(mi.params)\n",
    "mi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Now we can use `lmfit.minimize()` to run multiple Markov chains using the method `emcee`.  (You might have to install the `emcee` library using pip or conda.)  This is not actually a minimization but an MCMC chain.  Run 100 chains of lengths 2000 with burnin periord 500.  Use the minimum chi-square solution above as the initial guess.\n",
    "\n",
    "This might take a little while to run.  \n",
    "\n",
    "`emcee` can also be run outside of `lmfit`.  We are using `lmfit` here because it gives a nice way to fix different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lmfit.minimize(lnprob, method='emcee',\n",
    "                     nan_policy='omit',  # this removes non numerical outputs\n",
    "                     nwalkers=..., burn=..., steps=..., \n",
    "                     params=mi.params,\n",
    "                     progress=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Now we can make a nice \"corner plot\" using the package `corner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "result.params.pretty_print()\n",
    "labels=['$\\Omega_m$','$\\mu_o$','$\\Omega_\\Lambda$']\n",
    "fig = corner.corner(result.flatchain,labels=labels,\n",
    "                     show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **Correlation function for chains.**\n",
    "\n",
    "Now, we want to be sure the chain has converged.  We can do this by constructing the autocorrelation function:\n",
    "\\begin{align}\n",
    "C_{X}(n) = \\frac{\\frac{1}{N-n} ~\\sum_{i=0}^{N-n-1} \\left( X_i-\\overline{X} \\right) \\left(X_{i+n}-\\overline{X}\\right) }{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( X_i-\\overline{X} \\right)^2 }\n",
    "\\end{align}\n",
    "where $n$ is called the lag.  You can see that $C_X(0)=1$ by construction.  There is one of these for each parameter (and also crosscorrelation functions between parameters).\n",
    "\n",
    "The `result` object has attributes `result.chain` and `result.flatchain`.  The first is seporated into the different independent chains (i.e. different initial conditions) while the second is all the chains put together.  Look at their shapes with `np.shape()`. We will estimate the correlation function by averaging over the independent chains at the same lag.\n",
    "\n",
    "Calculate the correlation function from $n=0$ to $n=500$ for each of the 3 parameters. Plot $C(n)$ vs $n$.  It is helpful to use `np.cov()` instead of doing the sum above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.shape(result.chain)\n",
    "print(s)\n",
    "N=s[0]\n",
    "\n",
    "print(np.shape(result.chain[:,0,0]))\n",
    "lag = np.arange(0,500,5)\n",
    "\n",
    "## there will be three loops here plus a call to np.cov()\n",
    " # loop over parameters\n",
    " for v in range(s[2]) :\n",
    "    corr = np.zeros(len(lag))\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "     # loop over lags\n",
    "     for i,n in enumerate(lag) :\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        # loop over chains\n",
    "        for j in range(s[1]) :\n",
    "             c = np.cov(result.chain[0:N-n,j,v],result.chain[n:N,j,v])\n",
    "            .\n",
    "            .\n",
    "            \n",
    "    plt.plot(lag,corr,label=variable_names[v])\n",
    "\n",
    "plt.legend()\n",
    "plt.plot([0,lag[-1]],[0,0],linestyle=':')\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('correlation function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) If we define the correlation length as the first time the autocorrelation function hits zero, what is this length for $\\Omega_\\Lambda$ and how many correlation lengths are in each chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **The maximum likelihood**\n",
    "\n",
    "Find the entry in the chain with the largest likelihood.  The calculated log-likelihoods as stored in `result.lnprob` (np.argmax() is useful for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_prob = np.argmax(result.lnprob)\n",
    "print(highest_prob)\n",
    "hp_loc = np.unravel_index(highest_prob, result.lnprob.shape)\n",
    "print(hp_loc)\n",
    "mle_soln = result.chain[hp_loc]\n",
    "for i, par in enumerate(p):\n",
    "    p[par].value = mle_soln[i]\n",
    "\n",
    "print('\\nMaximum Likelihood Estimation from emcee       ')\n",
    "print('-------------------------------------------------')\n",
    "print('Parameter  MLE Value   Median Value   Uncertainty')\n",
    "fmt = '  {:5s}  {:11.5f} {:11.5f}   {:11.5f}'.format\n",
    "for name, param in p.items():\n",
    "    print(fmt(name, param.value, res.params[name].value,\n",
    "              res.params[name].stderr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) We can find quantile ranges for the parameters from `result.flatchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nError estimates from emcee:')\n",
    "print('------------------------------------------------------')\n",
    "print('Parameter  -2sigma  -1sigma   median  +1sigma  +2sigma')\n",
    "\n",
    "for name in p.keys():\n",
    "    quantiles = np.percentile(result.flatchain[name],\n",
    "                              [2.275, 15.865, 50, 84.135, 97.275])\n",
    "    median = quantiles[2]\n",
    "    err_m2 = quantiles[0] - median\n",
    "    err_m1 = quantiles[1] - median\n",
    "    err_p1 = quantiles[3] - median\n",
    "    err_p2 = quantiles[4] - median\n",
    "    fmt = '  {:5s}   {:8.4f} {:8.4f} {:8.4f} {:8.4f} {:8.4f}'.format\n",
    "    print(fmt(name, err_m2, err_m1, median, err_p1, err_p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) What is the 99% lower credibility limit on $\\Omega_\\Lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) What is posterior probability of $\\Omega_m$ being less than 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "14) **Compare a flat model to an open model**\n",
    "\n",
    "The geometrically flat Universe is where $\\Omega_m + \\Omega_\\Lambda=1$.  Thus, a flat model will have one less parameter ($\\Omega_\\Lambda$ will not be necessary).  Rewrite `mu_model(p,z)`, but now using `cosmo.FlatLambdaCDM()`, redefine `params` to have only two parameters and then find the minimum chi-squared solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_model(p,z):\n",
    "    ...\n",
    "    \n",
    "params = ...\n",
    "\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Does the Bayesian information criterion (BIC) support the conclusion that the Universe is not flat?  (Note that the BIC output when method='emcee' is not valid.  Calculate is yourself.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) Redo the MCMC with this two-parameter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17) Make a corner plot for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18) What is the 99% lower credibility limit on $\\Omega_\\Lambda$ within flat models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
