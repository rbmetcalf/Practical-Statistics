{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial 5 - Monte Carlo and Bootstrap**\n",
    "\n",
    "In this tutorial we will use the techniques of Monte Carlo simulation and Bootstrap resampling to estimate the errors in a statistic.\n",
    "\n",
    "In some situations a statistic might have a known distribution, but in other cases it is not possible to analytically derive the distribution of the statistic.  If we know the statistical distribution of the data and we can simulate data from it numerically, then we can find the distribution of our statistic by repeatedly calculating it on simulated data sets.  This is called a Monte Carlo (MC) simulation or a parametric bootstrap.\n",
    "\n",
    "The idea is to estimate the expectation value of any statistic $f(\\{x\\})$ with\n",
    "\\begin{align}\n",
    "E[f] \\simeq E_{\\rm mc}[f] \\equiv \\frac{1}{N_{\\rm mc}} \\sum_i^{N_{\\rm mc} } f\\left( \\{ x  \\}_i  \\right)\n",
    "\\end{align}\n",
    "\n",
    "where the $\\{ x \\}_i$ is the $i$ th simulated data set.  This can be used to find the bias and variance of any statistic as long as you can generate random samples from the distribution.  The estimated bias for a parameter $\\theta$ is \n",
    "\\begin{align}\n",
    "{\\rm bias}_\\theta = E_{\\rm mc}[\\hat{\\theta}] - \\theta\n",
    "\\end{align}\n",
    "where $\\theta$ is the parameter value used in generating the MC samples and $\\hat{\\theta}$ is an estimator of that parameter.\n",
    "\n",
    "An estimate for the variance of this estimator will be\n",
    "\\begin{align}\n",
    "{\\rm Var}_{\\rm mc}[\\hat{\\theta}] &= \\frac{1}{(N_{\\rm mc}-1) }  \\sum_i^{N_{\\rm mc} } \n",
    "\\left( ~ \\hat{\\theta}\\left( \\{ x  \\}_i  \\right)  -  E_{\\rm mc}[\\hat{\\theta}] ~ \\right)^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian case:**\n",
    "First we will investigate the mean and median of a sample drawn from a Gaussian distribution.  In this case we have a library function that will generate deviates from a normal distribution.  We saw in lecture that the sample mean of a sample drawn from a normal distribution is normally distributed with a variance of $\\sigma^2/n$. We will verify this using a MC simulation.\n",
    "\n",
    " 1) Consider a data set of n=10  drawn from a $\\mathcal{N}(0,1)$ distribution.  \n",
    " Create a random data set and find the mean and median of that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Nsample = 10\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('mean = ', ...)\n",
    "print('median = ', ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the bias and variance of the sample mean and median by simulating \n",
    "  Nmc=1000 data sets each of size n=10.  Put the above into a loop to find the \n",
    "  bias and variance of the mean and median of sets of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Nmc = 1000\n",
    "means = np.empty(Nmc)\n",
    "medians = np.empty(Nmc)\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "    \n",
    "print('Var of means ', ...)\n",
    "print('Var of medians ', ...)\n",
    "\n",
    "# What is the prediction of theory for the variance of the mean?\n",
    "# Does the result of this MC experiment agree with theory?\n",
    "\n",
    "\n",
    "\n",
    "#Is the variance of the median larger or smaller than the mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to look at a somewhat more realistic example where the bias and variance of the statistic, in this case an estimator, is not known analytically.  The Schechter luminosity function is used to model the distribution of galaxy luminosities and many other things such as dark matter halo mass functions.  It is given by\n",
    "\\begin{align}\n",
    "n(L) = \\phi_* \\left( \\frac{L}{L_*}\\right)^\\alpha ~ e^{-L/L_*}~ \\frac{dL}{L_*}\n",
    "\\end{align}\n",
    "This has three parameters, $ \\phi_*$, $\\alpha$ and $L_*$.  For galaxies $\\alpha \\simeq -1.25$, but we will be using the case of $\\alpha = -0.3$ (This avoids the complication of it requiring a lower limit to be normalizable.).\n",
    "We want to measure the value of $L_*$.  The normalization $\\phi_*$ will not be relevant because we are interested in the distribution and not the over all density.  An estimator for $L_*$ is \n",
    "\\begin{align}\n",
    "\\hat{L}_* = \\left( \\frac{1}{(1+\\alpha) n}  \\sum_{i=1}^n L_i  \\right)\n",
    "\\end{align}\n",
    "where $L_i$ are the luminosities of each observed galaxy and $n$ is the number of galaxies in the data set.  We want to know if this estimator is biased and what its variance is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this case we will build a function to generate random deviates from this distribution.\n",
    "\n",
    "3) Make a function that takes the luminosity in units of $L_*$, $ x = L/L_*$, and returns the properly normalized pdf for galaxy luminosities.  Take the minimum luminosity to be $x_{min} = 0.0$.\n",
    "You will find the following integrals useful\n",
    "\\begin{align}\n",
    "\\Gamma(\\alpha+1 ) =\\int^{\\infty}_{0} dx ~x^\\alpha ~ e^{-x}\n",
    "\\end{align}\n",
    "where $\\Gamma(\\alpha+1) $ is the gamma function.  The \n",
    "incomplete gamma function is\n",
    "\\begin{align}\n",
    "\\Gamma(\\alpha+1,c, b) =\\int^{b}_{c} dx ~x^\\alpha ~ e^{-x}\n",
    "\\end{align}\n",
    "These can be calculated using the mpmath library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lstar = 1.0e11\n",
    "alpha = -0.3\n",
    "\n",
    "import mpmath as mp\n",
    "\n",
    "def schechter(x) :\n",
    "    return ....\n",
    "\n",
    "## vectorize the function so that it will take a vector of x's\n",
    "schechterV = np.vectorize(schechter)\n",
    "\n",
    "# Make a plot of the pdf.  To make it look good you will probably need to \n",
    "# use plt.x(y)scale('log') to plot it in log scale. Label the axis.\n",
    "\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Make a function that returns the cumulative distribution for $x = L/L_*$.  \n",
    " Call it F.  This should not require summing the above.  You should write it \n",
    " using the incomplete gamma functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def F(xx) :\n",
    "    if(xx <= 0) :\n",
    "        return 0.0\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "FV = np.vectorize(F)\n",
    "\n",
    "## Plot the cumulative distribution.\n",
    "## It should go from 0 to 1 if you have used enough of a range in \n",
    "## x. Put some labels on it. Use a logarithmic x scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a function for generating random deviates from the Schechter distribution. We can do this by inverting the cumulative distribution to get the quantile function.  If we put uniform random deviates between 0 and 1 into this function we will get out random deviates drawn from the Schechter distribution.  \n",
    "\n",
    "We will invert the cumulative distribution numerically in this case by interpolating from a table of the cumulative distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5) First we need two arrays for pairs of $log(x)$ and $F(x)$. \n",
    "  \n",
    " We saw in the plot above that $F(x)$ is smooth if we use \n",
    " $log(x)$ so it is better to interpolate in $log(x)$\n",
    " instead of $x$. Because $log(x)$ is not defined at \n",
    " $x=0$ we also need to be sure that the \n",
    " interpolation table covers the range from \n",
    " $F(x) = 0$ to $F(x) = 1$ well enough that all likely \n",
    " values are represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays for interpolation within the quantile function\n",
    "lnx_int = np.arange(...) # evenly spaced in log\n",
    "x_int = np.exp(lnx_int)\n",
    "f_int = FV(x_int)\n",
    "# These arrays need to have different names from above \n",
    "# so that they don't change when the variables are reassigned.\n",
    "\n",
    "## Below is the code for inverting the cumulative \n",
    "## distribution \"by hand\".  You should understand \n",
    "## it and then make it more efficient by \n",
    "## uncommenting the numpy functions that do the \n",
    "## same thing internally.\n",
    "##\n",
    "def quantile(u) :\n",
    "    # if out of bounds\n",
    "    if(u <= f_int[0]) :\n",
    "        return lnx_int[0]\n",
    "    if(u >= f_int[-1]) :\n",
    "        return lnx_int[-1]\n",
    "    # find where u is in f array\n",
    "    i=0\n",
    "    ...\n",
    "    # interpolate to table of F(u) to get the quantile function.  Could contain np.interp()\n",
    "    return ...\n",
    "\n",
    "## Makes a vector version of the function\n",
    "def quantileV(u) :\n",
    "    ans = ...\n",
    "    ...\n",
    "    return ans\n",
    "#quantileV = np.vectorize(quantile)\n",
    "\n",
    "## Now we can draw randomly from the Schechter \n",
    "## distribution by passing uniform random numbers\n",
    "## into the quantile function.\n",
    "## Here are 1000 luminosities taken from the \n",
    "## distribution.\n",
    "\n",
    "u = np.random.uniform(0,1,1000)\n",
    "lnx = quantileV(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6) Verify that the random numbers created above \n",
    " do in fact come from the Schechter \n",
    " distribution by making the empirical cumulative \n",
    " distribution of them and over-plotting\n",
    " the cumulative distribution we found before in 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7) Now that we have a way of generating random luminosities we can\n",
    "   find the bias and variance of the estimator for $\\hat{L}_*$ given above.\n",
    "   Do 10000 random data sets of 20 galaxies each and find the bias and \n",
    "   variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmc = 10000\n",
    "Nsample = 20\n",
    "\n",
    "Lstar_estimates = np.empty(Nmc)\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('means Lstar', ...)\n",
    "print('bias', ...)\n",
    "print('standard deviation Lstar ', ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 8) If you increase the sample size does the bias \n",
    " and/or variance of the estimator change?  \n",
    " Make a plot of the bias and standard deviation \n",
    " as a function of the sample size from 3 to 200 \n",
    " skipping every 10 (i.e. Nsample = 3,13,23,..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Nmc = 1000\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "plt.plot( ... ,label='bias')\n",
    "plt.plot( ... ,label='standard deviation')\n",
    "plt.plot([0,200],[0,0],linestyle=':')\n",
    "plt.legend()\n",
    "plt.xlabel('sample size')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimator of $L_*$ is not very good when $\\alpha=-1.25$ (you can try it and see) and so should not be used to fit $L_*$ from real galaxy luminosity data.  The reason for this is that the luminosity function diverges at $L=0$ in this case.  As a result the selection function, which expresses the fact that we cannot detect galaxies that are below some luminosity, is always important and the estimator must be modified for this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an alternative estimator for $L_*$.  $F(x=1) = 0.76$ (where $x=L/L_*$) so taking the 0.76 quantile of the data might be a good estimator for $L_*$.  This is the data point for which 76\\% of the data has a smaller value and 24\\% of the data has a larger value.  (The 0.5 quantile is the median.)  You can find functions for doing this in numpy or sort the data yourself.  When there are too few data points (Nsample <~ 5) you can take the maximum data point. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 9) Write a function that takes the data and returns the new estimate for $L_*$ by returning the 0.76 quantile.\n",
    "\n",
    "Plot the bias and variance as a function of sample size as before. These are found by applying estimator to many samples drawn from the distribution.  Label each curve in the plot.\n",
    "\n",
    "Is this estimator biased?\n",
    "\n",
    "Does it have a lower or higher variance than the estimator we already considered for the same sample size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap errors ##\n",
    "\n",
    "Now let's apply the estimator to a data set, but this time we will use nonparametric bootstrap resampling to estimate the variance of the estimator.  Let us say that there is noise in the luminosity measurements and we do not know how it is distributed so we do not trust the Monte Carlo (parametric bootstrap) calculations we have done.\n",
    "\n",
    "Read in the luminosity data from the file `luminosities.csv`.  \n",
    "Make a loop that resamples this data 2500 times with bootstrap resampling each time. (hint: use np.random.randint() to make a new index each time.)\n",
    "\n",
    "Make a histogram of the estimated $L_*$ s for the bootstrap samples. Use the first estimator.\n",
    "\n",
    "Calculate the standard deviation of the estimated $L_*$ for these samples and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "df = pa.read_csv('luminosities.csv')\n",
    "\n",
    "Lave = np.mean(L)/(alpha + 1)\n",
    "n = 2500\n",
    "Lstars = np.zeros(n)\n",
    "for i in range(n) :\n",
    "    .\n",
    "    .\n",
    "    Lstars[i] = ...\n",
    "\n",
    "# make a histogram of lstars\n",
    ".\n",
    ".\n",
    "\n",
    "# find the mean and standard deviation\n",
    "print('L_* = ',... ,' +/- ',...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
