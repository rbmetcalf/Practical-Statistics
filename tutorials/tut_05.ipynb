{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial  \\# 5 Monte Carlo and Bootstrap #\n",
    "\n",
    "In this tutorial we will use the techniques of Monte Carlo simulation and Bootstrap resampling to estimate the errors in a statistic.\n",
    "\n",
    "In some situations a statistic might have a known distribution, but in other cases it is not possible to analytically derive the distribution of the statistic.  If we know the statistical distribution of the data and we can simulate data from it numerically, then we can find the distribution of our statistic by repeatedly calculating it on simulated data sets.  This is call a Monte Carlo (MC) simulation or a parametric bootstrap.\n",
    "\n",
    "The idea is to estimate the expectation value of any statistic $f(\\{x\\})$ with\n",
    "\\begin{align}\n",
    "E[f] \\simeq E_{\\rm mc}[f] \\equiv \\frac{1}{N_{\\rm mc}} \\sum_i^{N_{\\rm mc} } f\\left( \\{ x  \\}_i  \\right)\n",
    "\\end{align}\n",
    "\n",
    "where the $\\{ x \\}_i$ is the $i$th simulated data set.  This can be used to find the bias and variance of any statistic as long as you can generate random samples from the distribution.  The estimated bias for a parameter $\\theta$ is \n",
    "\\begin{align}\n",
    "{\\rm bias}_\\theta = E_{\\rm mc}[\\hat{\\theta}] - \\theta\n",
    "\\end{align}\n",
    "where $\\theta$ is the parameter value used in generating the MC samples and $\\hat{\\theta}$ is an estimator of that parameter.\n",
    "\n",
    "An estimate for the variance of this estimator will be\n",
    "\\begin{align}\n",
    "{\\rm Var}_{\\rm mc}[\\hat{\\theta}] &= \\frac{1}{(N_{\\rm mc}-1) }  \\sum_i^{N_{\\rm mc} } \n",
    "\\left( ~ \\hat{\\theta}\\left( \\{ x  \\}_i  \\right)  -  E_{\\rm mc}[\\hat{\\theta}] ~ \\right)^2 \\\\\n",
    "&= \\frac{1}{(N_{\\rm mc}-1) }  \\sum_i^{N_{\\rm mc} } \n",
    "\\left[~  \\hat{\\theta}\\left( \\{ x  \\}_i  \\right)  ~\\right]^2  - \\frac{N_{\\rm mc}}{(N_{\\rm mc}-1) }  E_{\\rm mc}[\\hat{\\theta}]^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian case:**\n",
    "First we will investigate the mean and median of a sample drawn from a Gaussian distribution.  In this case we have a library function that will generate deviates from a normal distribution.  We saw in lecture that the sample mean of a sample drawn from a normal distribution is normally distributed with a variance of $\\sigma^2/n$. We will verify this using a MC simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Consider a data set of n=10  drawn from a N(0,1) distribution.  \n",
    "# Create a random data set and find the mean and median of that data set.\n",
    "\n",
    "Nsample = 10\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('mean = ', ...)\n",
    "print('median = ', ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Find the bias and variance of the sample mean and median by simulating \n",
    "#  N_mc=1000 data sets each of size n=10.  Put the above into a loop to find the \n",
    "#  bias and variance of the mean and median of sets of 10.\n",
    "\n",
    "Nmc = 1000\n",
    "means = np.empty(Nmc)\n",
    "medians = np.empty(Nmc)\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "    \n",
    "print('Var of means ', ...)\n",
    "print('Var of medians ', ...)\n",
    "\n",
    "# What is the prediction of theory for the variance of the mean?\n",
    "# Does the result of this MC experiment agree with theory?\n",
    "\n",
    "\n",
    "\n",
    "#Is the variance of the median larger or smaller than the mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to look at a somewhat more realistic example where the bias and variance of the statistic, in this case an estimator, is not known analytically.  The Schechter luminosity function is used to model the distribution of galaxy luminosities and many other things such as dark matter halo mass functions.  It is given by\n",
    "\\begin{align}\n",
    "n(L) = \\phi_* \\left( \\frac{L}{L_*}\\right)^\\alpha ~ e^{-L/L_*}~ \\frac{dL}{L_*}\n",
    "\\end{align}\n",
    "This has three parameters, $ \\phi_*$, $\\alpha$ and $L_*$.  For galaxies $\\alpha \\simeq -1.25$, but we will be using the case of $\\alpha = -0.3$ (This avoids the complication of it requiring a lower limit to be normalizable.).\n",
    "We want to measure the value of $L_*$.  The normalization $\\phi_*$ will not be relevant because we are interested in the distribution and not the over all density.  An estimator for $L_*$ is \n",
    "\\begin{align}\n",
    "\\hat{L}_* = \\left( \\frac{1}{(1+\\alpha) n}  \\sum_{i=1}^n L_i  \\right)\n",
    "\\end{align}\n",
    "where $L_i$ are the luminosities of each observed galaxy and $n$ is the number of galaxies in the data set.  We want to know if this estimator is biased and what its variance is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we do not have a library function to get random deviates from this distribution so we will have to build one ourselves.\n",
    "\n",
    "3) Make a function that takes the luminosity $ x = L/L_*$ and returns the properly normalized pdf for galaxy luminosities.  Take the minimum luminosity to be $x_{min} = 0.0$.\n",
    "You will find the following integrals useful\n",
    "\\begin{align}\n",
    "\\Gamma(\\alpha+1 ) =\\int^{\\infty}_{0} dx ~x^\\alpha ~ e^{-x}\n",
    "\\end{align}\n",
    "where $\\Gamma(\\alpha+1) $ is the gamma function.  The \n",
    "incomplete gamma function is\n",
    "\\begin{align}\n",
    "\\Gamma(\\alpha+1,c, b) =\\int^{b}_{c} dx ~x^\\alpha ~ e^{-x}\n",
    "\\end{align}\n",
    "These can be calculated using the mpmath library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lstar = 1.0e11\n",
    "alpha = -0.3\n",
    "\n",
    "import mpmath as mp\n",
    "\n",
    "def schechter(x) :\n",
    "    return ....\n",
    "\n",
    "## vectorize the function so that it will take a vector\n",
    "schechterV = np.vectorize(schechter)\n",
    "\n",
    "# Make a plot of the pdf.  To make it look good you will probably need to \n",
    "# use plt.x(y)scale('log') to plot it in log scale. Label the axis.\n",
    "\n",
    "lnx = np.arange(-7,2,0.1) # evenly spaced in log\n",
    "x = np.exp(lnx)\n",
    "\n",
    "p = schechterV(x)\n",
    "L = x*Lstar\n",
    "\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Make a function that returns the cumulative distribution for x = L/Lstar.  \n",
    "# Call it F.  This should not require summing the above.  You should write it \n",
    "# using the incomplete gamma functions.\n",
    "\n",
    "def F(xx) :\n",
    "    if(xx <= 0) :\n",
    "        return 0.0\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "FV = np.vectorize(F)\n",
    "\n",
    "## Plot the cumulative distribution.\n",
    "## It should go from 0 to 1 if you have used enough of a range in \n",
    "## x. Put some labels on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a function for generating random deviates from the Schechter distribution. We can do this by inverting the cumulative distribution\n",
    " to get the quantile function.  If then put uniform random deviates between 0 and 1 into this function we will get out random deviates drawn from the Schechter distribution.  We will need to invert the cumulative distribution numerically in this case because it cannot be done analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) First we need two arrays for pairs of log(x) and F(x).  \n",
    "# We saw in the plot above that F(x) is smooth if we use \n",
    "# log(x) so it is better to interpolate in log(x)\n",
    "# instead of x. Because log(x) is not defined at \n",
    "# x=0 we also need to be sure that the \n",
    "# interpolation table covers the range from \n",
    "# F(x) = 0 to F(x) = 1 well enough that all likely \n",
    "# values are represented. \n",
    "\n",
    "# Arrays for interpolation within the quantile function\n",
    "lnx_int = np.arange(...) # evenly spaced in log\n",
    "x_int = np.exp(lnx_int)\n",
    "f_int = FV(x_int)\n",
    "# These arrays need to have different names from above \n",
    "# so that they don't change when the variables are reassigned.\n",
    "\n",
    "## Below is the code for inverting the cumulative \n",
    "## distribution \"by hand\".  You should understand \n",
    "## it and then make it more efficient by \n",
    "## uncommenting the numpy functions that do the \n",
    "## same thing internally.\n",
    "##\n",
    "def quantile(u) :\n",
    "    # if out of bounds\n",
    "    if(u <= f_int[0]) :\n",
    "        return lnx_int[0]\n",
    "    if(u >= f_int[-1]) :\n",
    "        return lnx_int[-1]\n",
    "    # find where u is in f array\n",
    "    i=0\n",
    "    while(f_int[i] < u):\n",
    "        i += 1\n",
    "    i -= 1\n",
    "    return lnx_int[i] + (u-f_int[i])*(lnx_int[i+1]-lnx_int[i])/(f_int[i+1]-f_int[i])\n",
    "    #return np.interp(u,f_int.astype('float'),lnx_int.astype('float'))\n",
    "\n",
    "## This makes a vector version of the function\n",
    "def quantileV(u) :\n",
    "    ans = np.empty(len(u))\n",
    "    for i in range(len(u)) :\n",
    "        ans[i] = quantile(u[i])\n",
    "    return ans\n",
    "#quantileV = np.vectorize(quantile)\n",
    "\n",
    "## Now we can draw randomly from the Schechter \n",
    "## distribution by passing uniform random numbers\n",
    "## into the quantile function.\n",
    "## Here are 1000 luminosities taken from the \n",
    "## distribution.\n",
    "\n",
    "u = np.random.uniform(0,1,1000)\n",
    "lnx = quantileV(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Verify that the random numbers created above \n",
    "# do in fact come from the Schechter \n",
    "# distribution by making the empirical cumulative \n",
    "# distribution of them and over-plotting\n",
    "# the cumulative distribution we found before in 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Now that we have a way of generating random luminosities we can\n",
    "#   find the bias and variance of the estimator for L* given above.\n",
    "#   Do 10000 random data sets of 20 galaxies each and find the bias and \n",
    "#   variance.\n",
    "\n",
    "Nmc = 10000\n",
    "Nsample = 20\n",
    "\n",
    "Lstar_estimates = np.empty(Nmc)\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('means Lstar', ...)\n",
    "print('bias', ...)\n",
    "print('standard deviation Lstar ', ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) If you increase the sample size does the bias \n",
    "# and/or variance of the estimator change?  \n",
    "# Make a plot of the bias and standard deviation \n",
    "# as a function of the sample size from 3 to 200 \n",
    "# skipping every 10 (i.e. Nsample = 3,13,23,..)\n",
    "\n",
    "Nmc = 1000\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "plt.plot( ... ,label='bias')\n",
    "plt.plot( ... ,label='standard deviation')\n",
    "plt.plot([0,200],[0,0],linestyle=':')\n",
    "plt.legend()\n",
    "plt.xlabel('sample size')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimator of $L_*$ is not very good when $\\alpha=-1.25$ (you can try it and see) and so should not be used to fit $L_*$ from real galaxy luminosity data.  The reason for this is that the luminosity function diverges at $L=0$ in this case.  As a result the selection function is always important and the estimator must be modified for this.  We will discuss better methods for these cases later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **extra credit** consider an alternative estimator for $L_*$.  $F(x=1) = 0.76$ ($x=L/L_*$) so taking the 0.76 quantile of the data might be a good estimator for $L_*$.  This is the data point for which 76\\% of the data has a smaller value and 24\\% of the data has a larger value.  (The 0.5 quantile is the median.)  You can find functions for doing this in numpy or sort the data yourself.  When there are too few data points (Nsample <~ 5) you can take the maximum data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9)\n",
    "\n",
    "# write a function that takes the data and returns the new estimate \n",
    "# for L_*.\n",
    "\n",
    "# plot the bias and variance as a function of sample size as before.\n",
    "\n",
    "# Is this estimator biased?\n",
    "\n",
    "#Does it have a lower or higher variance than the estimator we \n",
    "#already considered for the same sample size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap errors ##\n",
    "\n",
    "Now let's apply the estimator to a data set, but this time we will use bootstrap resampling to estimate the variance of the estimator.  There is noise in the luminosity measurements and we do not know how it is distributed.\n",
    "\n",
    "Read in the luminosity data from the file `luminosities.csv`.  \n",
    "Make a loop that resamples this data 3000 times with bootstrap resampling each time. (hint: use np.random.randint() to make a new index each time)\n",
    "\n",
    "Make a histogram of the estimated $L_*$s for the bootstrap samples. \n",
    "\n",
    "Calculate the standard deviation of the estimated $L_*$ for these samples and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "df = pa.read_csv('luminosities.csv')\n",
    "\n",
    "Lave = np.mean(L)/(alpha + 1)\n",
    "n = 3000\n",
    "lstars = np.zeros(n)\n",
    "for i in range(n) :\n",
    "    .\n",
    "    .\n",
    "    lstars[i] = np.mean(L[index])/(alpha + 1)\n",
    "\n",
    "# histogram of lstars\n",
    ".\n",
    ".\n",
    "print('L_* = ',... ,' +/- ',...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
