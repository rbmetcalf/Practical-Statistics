{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial 6 - Cosmology with Type Ia Supernovae : linear models**\n",
    "\n",
    "In this tutorial we will experiment with some type Ia supernova data.  This is part of the data used to determine that Universe's expansion is accelerating.\n",
    "\n",
    "Data obtained is from the Supernova Cosmology Project at:\n",
    "http://supernova.lbl.gov/union/descriptions.html#Magvsz\n",
    "\n",
    "Background:\n",
    "\n",
    "The observed magnitude of an object with luminosity $L$ is\n",
    "\n",
    "$m = - 2.5 \\log\\left( \\frac{L}{2\\pi D_L^2} \\right) + m_o = 5 \\log\\left( D_L \\right) + 2.5 \\log\\left( L \\right) + m_o$\n",
    "\n",
    "where $D_L$ is the luminosity distance.  The peak luminosity of a type Ia supernovae \n",
    "is directly related to the width of its lightcurve and its color.  In this data set, the correction to a standard candle has already been done (we will look at this in a later tutorial) and it is reported in terms of the estimated distance modulus\n",
    "\n",
    "$\\mu = 5 \\log\\left( D_L \\right) - 5$\n",
    "\n",
    "This assumes a Hubble constant and requires a calibration using other distance indicators in local galaxies so there is an additive constant to the distance modulus or a multiplicative constant to the brightness that is not very well constrained, ie the relative brightnesses of the supernovae are well measured but not their absolute brightnesses.\n",
    "\n",
    "General relativity and the energy content of the Universe predicts $D_L(z)$ or $\\mu(z)$ where $z$ is the cosmological redshift of the supernovae.\n",
    "\n",
    "In this tutorial we will make and test some linear models for $\\mu(z)$.  We will also see if a flat Universe without a cosmological constant is an adequate fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import the data using the command pandas.read_csv(\"SCPUnion2.1_mu_vs_z.txt\",sep='\\t',comment='#').\n",
    "\n",
    "Plot the distance modulus vs redshift with error bars.  Label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Try fitting this curve with a polynomial models of order 1,2 and 3 using numpy.polyfit(). Use the w parameter to include the errors from the data files in these $\\chi^2$ fits.  Plot these models on top of the data.  Leave the errors out to improve clarity.  Do these make good models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Convert the distance modulus into luminosity distance\n",
    "\n",
    "$D_L = A 10^{\\frac{\\mu}{5}}$\n",
    "\n",
    "were we don't know the constant $A$ because of calibration and uncertainties in the Hubble constant.\n",
    "\n",
    "Fit a second order polynomial to $D_L$ vs $z$.  You can estimate the errors in the distance using the expansion\n",
    "\n",
    "$ \\delta D_L \\sim \\frac{\\ln(10)}{5} ~ D_L \\delta \\mu$\n",
    "\n",
    ",the first order expansion of $D_L$ in terms of $\\mu$.  This is only an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) General relativity and the standard cosmological model predict that to \n",
    "second order\n",
    "\n",
    "$D_L(z) \\simeq \\frac{c}{H_o} \\left[ z + \\frac{z^2}{2}(1-q_o) \\right]$\n",
    "\n",
    "where $q_o$ is called the deceleration parameter and is given by\n",
    "\n",
    "$q_o = \\frac{\\Omega_m}{2} - \\Omega_\\Lambda$\n",
    "\n",
    "where $\\Omega_m$ is the density of the Universe in ordinary matter in units of the critical density and $\\Omega_\\Lambda$ is the energy density of the cosmological constant in the same units.\n",
    "\n",
    "If the Universe is geometrically flat $\\Omega_m+ \\Omega_\\Lambda = 1$ and \n",
    "\n",
    "$q_o = \\frac{3}{2}\\Omega_m - 1$\n",
    "\n",
    "If there is no cosmological constant (the expansion is not accelerating) $q_o=1/2$.\n",
    "\n",
    "This means that in your model $p[0]/p[1] = (1-q_o)/2$\n",
    "\n",
    "For a flat universe with no cosmological constant $\\Omega_m = 1$, $\\Omega_\\Lambda =0$  $p[0]/p[1] = 1/4$.  \n",
    "\n",
    "What is your measured value for $q_o$?  If the universe is flat what is your measured value for $\\Omega_m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) This looks good, but the errors are not really on $D_L$.  They are on \n",
    "the distance modulus.  We have also not enforced the requirement that $D_L(z=0) =0$. This was not possible with numpy.polyfit() because it will only fit a polynomial and requires that the intercept be free.\n",
    "\n",
    "What we really want to find is a nonlinear model.  For this lets use scipy.optimize.curve_fit().  For this we must \n",
    "\n",
    "a) Define a function func_modulus($z,\\mu_o,q_o$) which returns \n",
    "\n",
    "$5\\log_{10}\\left(z + \\frac{1}{2}(1-qo)z^2\\right) + \\mu_o$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use scipy.optimize.curve_fit() to find the model.  Use the sigma and absolute_sigma=True options to include the errors.\n",
    "\n",
    "The outputs of this function are model,covariance.\n",
    "\n",
    "What is the $q_o$ you get in this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use func_modulus(z,muo,qo) to plot the best fit model over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Calculate the $\\chi^2$ for this model and data.  Remember that \n",
    "\n",
    "$\\chi^2 = \\sum_i \\frac{ \\left[ y_i - model(x_i) \\right]^2}{\\sigma^2} $\n",
    "\n",
    "Is this an acceptable fit?  Use the $\\chi^2$ distribution to find out.  How many degrees of freedom should you use? (hint: use scipy.stats.chi2.cdf() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We need to calculate the errors on our measurement of $q_o$.  We will do this in two ways.  \n",
    "\n",
    "Find the bootstrap variance of your estimate of $q_o$.  This will require you to make a new data sets by sampling from the original one with replacement (Hint: numpy.random.choice(N,N) will give you a random set of integers between 0 and N-1).  Then find $q_o$ from this data set.  Put this in a loop and repeat 1000 times recording the $q_o$ value each time in a vector of $q_o$'s. \n",
    "\n",
    "Make a nice histogram of the $q_o$ vector.\n",
    "\n",
    "This vector of $q_o$'s is approximately sampled from the true distribution.  Find the mean and variance of this vector.  This is a rough estimate of the variance.\n",
    "\n",
    "What is the measured value of $q_o$ with errors?\n",
    "\n",
    "Assuming the Universe is flat what is the measured value of $\\Omega_m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**  In this tutorial we have made several approximations that might not be justified.  One is that we use an expansion for $D_L$ as a function of $z$ that is not really valid at z ~ 1.  If we use the correct relationship and calculate the errors in a better way we might get a different answer for $\\Omega_m$.  We will do this in our next tutorial.\n",
    "\n",
    "Also another very useful program for doing linear fitting is sklearn.linear_model.LinearRegression().  \n",
    "It is more prediction oriented, but you might find it useful especially when you have multiple independant and \n",
    "dependant variables.\n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
