{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Covariance and Principle Components\n",
    "\n",
    "In this tutorial we will get some experience with estimating the covariance matrix and finding principle components of some data. You will also get some experience manipulating matrices in python.\n",
    "\n",
    "An estimator for the covariance matrix between variables $x$ and $y$ is\n",
    "\\begin{align}\n",
    "{\\hat{C}}_{xy} = \\frac{1}{N-1} \\sum_{i=1}^N \\left( x_i - \\bar{x} \\right) \\left( y_i - \\bar{y} \\right)\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to import numpy, matplotlib.pyplot and pandas\n",
    "\n",
    "\n",
    "1) Read file `homework_01_2d-datafile.csv` into a dataframe using pandas\n",
    "\n",
    "Make a scatter plot of X vs Y.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2) Find the covariance matrix for the two variables.  Don't use the \n",
    " function numpy.cov(), write your own function this first time.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the covariance matrix C for X and Y.\n",
    "\n",
    "# Do X and Y appear to be correlated?\n",
    "\n",
    "# What is the variance of X?\n",
    "\n",
    "# What is the variance of Y?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Find the percision matrix, $C^{-1}$, and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized covariance coefficients, or Pearson's correlation coefficients,  are\n",
    "\\begin{align}\n",
    "\\rho_{ij} = \\frac{\\hat{C}_{ij} }{\\sqrt{\\hat{C}_{ii} \\hat{C}_{jj} }}\n",
    "\\end{align}\n",
    "\n",
    "The off-diagonal components of this matrix are measures of correlations.  They have a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4) Consider the following code:\n",
    " \n",
    " ```\n",
    "    D = np.diag(C)\n",
    "    \n",
    "    print(D)\n",
    "    \n",
    "    J =  np.outer(D,D)\n",
    "    \n",
    "    print(J)\n",
    "```\n",
    " Use it to efficiently calculate the denominator for the normalized covariance matrix and print the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5) Decompose the covariance (not normalized) matrix using \n",
    "    an eigenvalue decompositions.\n",
    "    \n",
    "    Use V,M = numpy.linalg.eig() to find the decomposition.  V contains the eigenvalues and M contains the eigenvalues vectors as columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "    # What are the principle components (eigenvectors) of the data? \n",
    " .\n",
    " .\n",
    " .   \n",
    "    print(\"v1 = \",...)\n",
    "    print(\"v2 = \",...)\n",
    "\n",
    "\n",
    "    # What are the eigenvalues?\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6) Transform the data into the basis of the principle components (or eigenvectors) that were found in 5).  If `M` is a matrix whose *rows* are the eigenvectors of `C` then `x=M d` is the tranformation of the data point `d` into its principle components `x`.\n",
    "\n",
    "  You can collect the data into a structure `data = np.array([X,Y]`) and then matrix \n",
    "  multiply it by a matrix with `np.dot(,)` (or `@`). You might need to transpose something...\n",
    "  \n",
    "`numpy.shape()` is useful to make sure your doing matrix multiplications correctly when the matrices are not square.  The trick here is to remember what dimensions things should be.  This should take only a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a scatter plot of the data in this basis\n",
    "\n",
    "#print the covariance matrix of the data in this basis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the diagonal elements (variances) of the covariance matrix \n",
    "to the eigenvalues you got in part 4).  Are the variances of the principle components larger, smaller or both \n",
    "larger and smaller than the original variances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7) Do 1) through 6), but using the data file `homework_01_5d-datafile.csv` this time.\n",
    "    In this case the data is 5 dimensional.  You can use numpy.cov() this time.\n",
    "    You can't plot all those dimensions so you don't have to do the scatter plots\n",
    "    ,but feel free to plot some dimensions to make sure you are getting what \n",
    "    you expect.\n",
    "\n",
    " The best way to find the covariance matrix is to make a 5 by 2000 array out \n",
    " of the columns of the dataframe using `numpy.array([...])` as we did in the 2D case above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Which variables seem to be correlated with each other and which ones not?\n",
    " \n",
    "\n",
    "    # What are the principle components (eigenvectors) of the data? \n",
    "\n",
    "\n",
    "    # What are the variances along each principle component?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
