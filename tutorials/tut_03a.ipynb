{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b5867a",
   "metadata": {},
   "source": [
    "**Tutorial 3a - Measuring the Properties of an Eclipsing Extrasolar Planet**\n",
    "\n",
    "In this tutorial you will measure the parameters of the light curve of an eclipse of a star by a planet.  We will do this using Bayesian statistical inference.\n",
    "\n",
    "We will first find the best-fit model parameters which in this case is the maximum likelihood.  We will then find the contingent posteriors for three of the parameters.  Finally we will find the 2 dimensional posterior marginalized over a third parameter.\n",
    "\n",
    "The data consists of measured fluxes of a star at different times.  As the planet passes in front of the star there is a dip in the flux.  The important quantities to measure are : 1) What is the baseline flux before and after the eclipse ; 2) How deep this dip which is related to how large the planet is compared to the star ; 3) How long the eclipse is which is related to the orbital speed and sizes of the star and planet ; 4) How steep are the transition into and out of the eclipse which is related to the limb darkening of the star combined with the size and speed of the planes.  These will be parameters that will be found from the light curve and later interpreted in terms of physical quantities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8be15",
   "metadata": {},
   "source": [
    "1) First you must import `numpy`, `pandas`, `matplotlib.pyplot` and `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaca87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67525481",
   "metadata": {},
   "source": [
    "2) Now import the data file \"eclipsing_planet_data.csv\" into a dataframe.  This file has the measured fluxes, the time of each observation and the estimated errors in each observation.  Put these into separate vector `t`, `f` and `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0cebad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48758ab1",
   "metadata": {},
   "source": [
    "3) Plot time vs flux.  The units are in minutes and arbitrary relative flux units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7a53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee974ef",
   "metadata": {},
   "source": [
    "4) We must build a model for this light curve.  We will use the model :\n",
    "    \n",
    "$ f(t) = f_o - \\delta F \\exp[- \\frac{|t - to|^n}{(\\Delta t)^n}  ]  $ \n",
    "\n",
    "The parameters are the baseline flux ($f_o$), the depth of the dip ($\\delta F$), the time of the eclipse ($t_o$), the width of the eclipse ($\\Delta t$) and the parameter $n$ which controls the speed of the drop into the eclipse.\n",
    "\n",
    "Write a function that takes first the time, $t$ and then the other parameters and returns the predicted flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_curve(t,Fo,dF,dt,to,n) :\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7d75c",
   "metadata": {},
   "source": [
    "5) We will assume the noise is Gaussian distributed.  We need to make a $\\chi^2$ function that we will minimize to find the best fit parameters.\n",
    "\n",
    "$\\chi^2(p) = \\sum_i \\frac{(f_i - L(t_i,p) )^2}{\\sigma_i^2}$\n",
    "\n",
    "Where $L(t_i,p)$ is the model for the lightcurve.  Make a function that returns the $\\chi^2$ and takes the parameters as a vector $p$.  These are the same parameters ($f_o$, $\\delta F$, $\\Delta t$, $t_o$, $n$), but they need to be encoded in a vector to work with the optimizer. Since `t` and `f` are in global memory they can be used inside this function and don't need to be arguments.\n",
    "\n",
    "Write this $\\chi^2(p)$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c047fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2(p):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184af8d",
   "metadata": {},
   "source": [
    "6) Now we can find the set of parameters that minimises $\\chi^2(p)$.  To do this we will use `scipy.optimize`.  In this function use `method = 'Nelder-Mead'`.  You will need to make an initial guess as to what these parameters are.  Look at the data and estimate them.  Check the output of the optimizer.  If it has not converged properly your guess was not good and you should try again (or there is some other problem).  Save the results in `best_fit`. \n",
    "\n",
    "Print out the best fit parameters with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = ...\n",
    "best_fit = ...\n",
    "\n",
    "print('best fit model :')\n",
    "print('    Fo = ',best_fit.x[0],'flux')\n",
    "print('    dF = ',best_fit.x[1],' flux')\n",
    "print('    dt  = ',best_fit.x[2],' mins')\n",
    "print('    to = ',best_fit.x[3],'  mins')\n",
    "print('    n = ',best_fit.x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875c625",
   "metadata": {},
   "source": [
    "7) Make a new vector of times that is evenly spaced.  Use `light_curve()` to make a model lightcurve with the best-fit parameters.  Plot this and the data together.  Make sure it looks like a reasonable fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa599834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cadcd59b",
   "metadata": {},
   "source": [
    "8) Now we want to find the *conditional* posterior distribution for the centeral time $t_o$ with a uniform prior distribution.  \n",
    "\n",
    "Set the all the other parameters to the best fit values you have already found.  Make a loop through possible $t_o$s.  At each value calculate the $\\chi^2$ and store it in and array.  Because the noise is Gaussian distributed, the log of the likelihood is\n",
    "\n",
    "$\\ln L (t_o,F_o,\\delta F,\\Delta t,n) = - \\frac{1}{2} \\chi^2(t_o,F_o,\\delta F,\\Delta t,n) + const.$\n",
    "\n",
    "Use `np.trapz(L,x=tos)` to normalize the likelihood and plot it as a function of $t_o$.  Make sure you have used enough $t_o$s and a wide enough range that the integral is accurately calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee263f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = best_fit.x\n",
    "d = 0.25\n",
    "\n",
    "tos = np.arange(270,280,d)\n",
    "lnL = np.zeros(len(tos))\n",
    "i=0\n",
    "for tt in tos :\n",
    "    p[3] = tt\n",
    "    lnL[i] = ...\n",
    "    i += 1\n",
    "\n",
    "# normalize the likelihood to 1 using numpy.trapz() to do the integral \n",
    "....\n",
    "\n",
    "plt.plot(tos,L)\n",
    "plt.xlabel(r'$t_o$')\n",
    "plt.ylabel(r'$P(t_o | \\hat{F}_o,\\hat{\\delta F},\\hat{\\Delta t},\\hat{n})$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdb805",
   "metadata": {},
   "source": [
    "9) Calculate the mean and standard deviation of $P(t_o | \\hat{F}_o,\\hat{\\delta F},\\hat{\\Delta t},\\hat{n})$.  You sould do this by approximating an integral over the pdf as a sum over the tabulated values for the pdf that you have found in 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef844df8",
   "metadata": {},
   "outputs": [],
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('to = ',ave_to,' +/- ',np.sqrt(var_to))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c0458",
   "metadata": {},
   "source": [
    "10) Do the same for $P(\\delta F | \\hat{\\Delta t}, \\hat{F}_o,\\hat{t_o},\\hat{n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48059bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = best_fit.x\n",
    "d = 0.001\n",
    "dFs = np.arange(0.12,0.18,d)\n",
    "\n",
    "lnL = np.zeros(len(dFs))\n",
    "i=0\n",
    "for dF in dFs :\n",
    "  ...\n",
    "\n",
    "# normalize the likelihood to 1 using numpy.trapz() to do the integral \n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "plt.plot(dFs,L)\n",
    "plt.xlabel(r'$ \\delta F$')\n",
    "plt.ylabel(r'$P(\\hat{\\delta} | \\Delta t, \\hat{F}_o,\\hat{t_o},\\hat{n})$')\n",
    "plt.show()\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('dF = ',ave_dF,' +/- ',np.sqrt(var_dF))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6542f",
   "metadata": {},
   "source": [
    "11) Do the same for $P(\\Delta t | \\hat{F}_o,\\hat{\\delta F},\\hat{t_o},\\hat{n})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e005389",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = best_fit.x\n",
    "d = 0.1\n",
    "dts = np.arange(45,55,d)\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "# normalize the likelihood to 1 using numpy.trapz() to do the integral \n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "plt.xlabel(r'$\\Delta t$')\n",
    "plt.ylabel(r'$P(\\Delta t | \\hat{F}_o,\\hat{\\delta F},\\hat{t_o},\\hat{n})$')\n",
    "plt.show()\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print('Delta t = ',ave_dt,' +/- ',np.sqrt(var_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d52658",
   "metadata": {},
   "source": [
    "12) Now we want to find the posterior *marginalized over* $t_o$.  This involves integrating over $t_o$ for each value of $\\delta F$ and $\\Delta t$ :\n",
    "\n",
    "$P(\\delta F,\\Delta t | \\hat{F}_o,\\hat{n}) = \\int dt_o~ P(\\delta F,\\Delta t, t_o | \\hat{F}_o,\\hat{n})$\n",
    "\n",
    "We will keep $F_o$ and $n$ fixed to thier best fit values $\\hat{F}_o$ and $\\hat{n}$ to make things simpler.\n",
    "\n",
    "Make two nested loops over $\\delta F$ and $\\Delta t$ within which the likelihood is integrated over $t_0$.  To do this we must define a function that can be integrated by `scipy.integrate.quad`.  The first argument of this function is the variable over which the integral and the second is the other parameters.\n",
    "\n",
    "These likelihoods should be stored in a 2 dimentional array `L[i,j]`.  Then make contour plot of $L$.  This calculation might take a while.  If it takes too long you can reduce the resolution in $\\delta F$ and $\\Delta t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38582df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def integrand3(t_o,p):\n",
    "    p[3] = t_o\n",
    "    return np.exp(-0.5*chi2(p))\n",
    "\n",
    "\n",
    "\n",
    "p = ...\n",
    "L = ...\n",
    "j= ...\n",
    "for dF in dFs :\n",
    "    i= ...\n",
    "    for dt in dts :\n",
    "        p[1] = ..\n",
    "        p[2] = ..\n",
    "        L[i,j] = ..\n",
    "        i += ..\n",
    "    j += ..\n",
    "    \n",
    "plt.contour(dFs,dts,L)\n",
    "plt.xlabel(r'$\\delta$')\n",
    "plt.ylabel(r'$\\Delta F$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2375274c",
   "metadata": {},
   "source": [
    "So far we have calculated the un-normalized posterior.  We need to find the contours in parameter space that contain 68%, 95% and 99% of the posterior probability.\n",
    "\n",
    "Here is a function which given a tabulated distribution distribution `poterior` and a fraction will return the contour level within which that fraction of the distribution  is contained.  For example : The contour within which half of the probability is contained would be `find_level(probability_table,0.5)[0]`.  The function assumes that the `probability_table` is evaluated on a uniform (in prior probability) grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_level(prob,fraction) :\n",
    "    '''\n",
    "    this function finds the level for a \n",
    "    contour that contains a fixed fraction  \n",
    "    of the total sum of pixels\n",
    "    '''\n",
    "    tot = np.sum(prob)\n",
    "    max = np.max(prob)\n",
    "    min = np.min(prob)\n",
    "\n",
    "    level = 0.5*(max + min)\n",
    "    frac = np.sum( prob[ prob >= level ]  )/tot\n",
    "    res = np.min( prob[ prob >= level ]  )/tot\n",
    "\n",
    "    while( abs(frac - fraction) > res  ) :\n",
    "        \n",
    "        if( frac > fraction) :\n",
    "            min = level\n",
    "        else :\n",
    "            max = level\n",
    "            \n",
    "        level = 0.5*(max + min)\n",
    "            \n",
    "        frac = np.sum( prob[ prob >= level ] )/tot\n",
    "        res = np.min( prob[ prob >= level ]  )/tot\n",
    "\n",
    "    return level,frac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4edae4",
   "metadata": {},
   "source": [
    "13) Make a credibility plot for $\\delta F$-$\\Delta t$ with 68%, 95% and 99% contours.  The function above can be used to find the levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de065ee",
   "metadata": {},
   "outputs": [],
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "print(levs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "CS = ax.contour(dFs,dts,L,levs)\n",
    "## the stupidly complicated way contours are labeled \n",
    "fmt = {}\n",
    "strs = [ '99%', '95%', '68%']\n",
    "for i,s in zip( CS.levels, strs ):\n",
    "    fmt[i] = s\n",
    "ax.clabel(CS, inline=True, fontsize=10,fmt=fmt)\n",
    "\n",
    "plt.xlabel(r'$\\delta F$')\n",
    "plt.ylabel(r'$\\Delta t$')\n",
    "plt.title(r'$P(\\delta F,\\Delta t | \\hat{F}_o)$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
