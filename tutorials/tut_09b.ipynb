{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial 9b - Bayesian Model Checking**\n",
    "\n",
    "In this tutorial, we will do some Bayesian model checking on the type Ia supernovae data we used in tutorial 8b.\n",
    "\n",
    "The object of this exercise is to see if the data is consistent with the cosmological and statistical model we have been using to find the best-fit parameters.  These assumptions could be wrong in any number of ways.  For example, the errors in the distance moduli are not Gaussian or not measured correctly, the cosmological model we are assuming is incorrect or the observed redshifts which we have treated as an independent variable have significant errors in them.  We might wonder if the cosmological model we have assume which has no curvature (flat) and a cosmological constant is too simple.  Might there be evidence that the cosmological constant is evolving? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Go back to tutorial 8b.  \n",
    "\n",
    "After question 6) save the chain `result.flatchain` first as a Pandas `DataFrame` and then to a CSV file called `open_mcmc.csv` (df.to_csv(filename)).\n",
    "\n",
    "After question 16) save the chain `result.flatchain` first as a Pandas `DataFrame` and then to a CSV file called `flat_mcmc.csv`.\n",
    "\n",
    "Read in the MC chain of parameters that you calculated in tutorial 8b.  Put them into dataframes `open_mcmc` and `flat_mcmc` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_mcmc = pa.read_csv(...)\n",
    "flat_mcmc = ...\n",
    "\n",
    "print(open_mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Read in the supernova data again and put them in the arrays named."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pa.read_csv(\"SCPUnion2.1_mu_vs_z.txt\",sep='\\t',comment='#')\n",
    "\n",
    "redshifts = data['redshift']\n",
    "magnitudes = data['dist_mod']\n",
    "errors = data['dist_mod_error']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Make functions `mu_model_flat(p)` and `mu_model_open(p)` as was done in tutorial 8b.  But in this case they should use the array `redshifts` already defined and return a vector of predictions for each supernova.\n",
    "\n",
    "The parameters should be `p['M']`, `p['Omega_m']` and `p['Omega_Lambda']` in the case of `mu_model_open(p)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import astropy.cosmology as cosmo\n",
    "def mu_model_flat(p):\n",
    "    .\n",
    "    .\n",
    "\n",
    "def mu_model_open(p):\n",
    "    .\n",
    "    .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We will need to generate new mock data sets given a set of parameters.\n",
    "\n",
    "Make two functions that take the parameters `params` and generate a new data set with the same size and redshifts.  Inside the function it should use `mu_model_flat()` or `mu_model_flat()`.  You should add noise to each image.  Assuming the errors are normally distributed in magnitudes.  No looping should be necessary.  (hint: Use `numpy.random.normal()` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_flat(params) :\n",
    "    .\n",
    "    .\n",
    "\n",
    "def data_generator_open(params) :\n",
    "    .\n",
    "    .\n",
    "  \n",
    "\n",
    "# lets print a data set to see if it is working\n",
    "p = {'M':20,'Omega_m':0.3,'Omega_Lambda':0.1}\n",
    "print(data_generator_open(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now we need to define the $\\chi^2$ functions.\n",
    "\n",
    "Making `chi2_flat()` and `chi2_open()` functions that take the parameters and, as a keyword parameter `data`,  the distance moduli.  They should return the $\\chi^2$.  It should use the `errors` array and `mu_model()` inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chi2_flat(params,data=None) :\n",
    "    .\n",
    "    .\n",
    "    \n",
    "def chi2_open(...) :\n",
    "    .\n",
    "    .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Use the function `lmfit.minimize()` to minimize `chi2_flat()` and `chi2_open()` to find the best-fit parameters for the observed data.  Note that the object returned by `lmfit.minimize` has both the parameter values at the minimum (`result.params`).  Store these values for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit\n",
    "\n",
    "params = lmfit.Parameters()\n",
    "params.add_many((...\n",
    "           )\n",
    "\n",
    "result_flat = lmfit.minimize(chi2_flat, params, method='Nelder',kws={'data':magnitudes})\n",
    "lmfit.printfuncs.report_fit(result_flat.params)\n",
    "\n",
    "best_chi2_flat = ...\n",
    "\n",
    "params.add(...)\n",
    "\n",
    "result_open = ...\n",
    "lmfit.printfuncs.report_fit(result_open.params)\n",
    "\n",
    "best_chi2_open = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What is the difference in the minimum $\\chi^2$ 's for the two models?  Does this signify that one model should be favored over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Besides $\\chi^2$, we could also think of other statistics.  One that we will use is the maximum absolute residual with respect to the best-fit model prediction\n",
    "\n",
    "${\\rm max}_i\\left| \\frac{\\mu_i - \\mu_{model}(M,\\Omega_m,\\Omega_\\Lambda,z_i)}{\\sigma_i}  \\right|$\n",
    "\n",
    "We would expect that if the errors are not Gaussian in the sense that there are catastrophic outliers, this value would be higher than expected.\n",
    "\n",
    "Find the maximum absolute residual for the data set and store it in `max_res_observed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_max_res_observed = ...\n",
    "print(\"maximum absolute residual for data flat = \",flat_max_res_observed)\n",
    "\n",
    "open_max_res_observed = ...\n",
    "print(\"maximum absolute residual for data open = \",open_max_res_observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Now we have all the tools to create a sample drawn from the distribution\n",
    "\n",
    "$ p(T) = \\int d\\theta ~p(\\theta | D ) p(T | \\theta) = \\int d\\theta ~p(\\theta | D ) \\int_{V(T)}dx~ p(x | \\theta) $\n",
    "\n",
    "where $T$ is a statistic and $V(T)$ is the volume in data-space where $T(x)=T$.  \n",
    "The bootstrap approximation for this distribution is\n",
    "\n",
    "$p(T) \\simeq \\frac{1}{n} \\sum \\delta\\left( T - T(x_i) \\right) ~~~~ x_i \n",
    "\\sim p(x | \\theta) ~,~ \\theta \\sim p(\\theta | D ) $\n",
    "\n",
    "From this we can calculate the significance of the statistic $T(x)$ for our two models.\n",
    "\n",
    "\n",
    "For $T$, we will choose the minimum $\\chi^2$ and the maximum absolute residual.  \n",
    "\n",
    "Make at least 1000 simulated data sets, calculate the statistics, and store their values.\n",
    "Do this only for the open model.\n",
    "\n",
    "The parameters should be a random selection from the MCMC chain.  This should not involve the observed data directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import randint\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "chi2array = []\n",
    "max_res = []\n",
    "\n",
    "nmcmc = ...\n",
    "\n",
    "p = ...\n",
    "\n",
    "for i in range(1,1000) :\n",
    "    # take random set of parameters from the Markov Chain \n",
    "    params = open_mcmc.to_dict('records')[ randint(...) ]\n",
    "    print(i,params)\n",
    "    \n",
    "    # generate a new random data set from the model and this set of parameters \n",
    "    data = ...\n",
    "      \n",
    "    # find the maximum likelihood parameters for the new dataset \n",
    "    bestfit = lmfit.minimize(chi2_open, p, method='Nelder',kws={'data':data})\n",
    "\n",
    "    chi2array.append(...)\n",
    "    \n",
    "    max_res.append(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Make a histogram of the maximum absolute residuals (MAR).  Then make a plot of their cumulative distribution.  Mark, in this last plot, the observed MAR with a vertical line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0]. ...\n",
    "ax[0].set_xlabel(r'max(residual)')\n",
    "ax[0].set_box_aspect(aspect=1)\n",
    "\n",
    ".\n",
    ".\n",
    "\n",
    "ax[1].plot(...)\n",
    "ax[1].plot(...,[0,1],linestyle=':')\n",
    "ax[1].set_ylim(0,1)\n",
    "\n",
    "ax[1].set_xlabel(r'max(residual)')\n",
    "ax[1].set_ylabel(r'F$(max(residual))$')\n",
    "ax[1].set_box_aspect(aspect=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Make a histogram of the minimum $\\chi^2$ s from the simulations. Then make a plot of their cumulative distribution. Mark, in this last plot, the observed minimum $\\chi^2$ with a vertical line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very similar to above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Calculate the right-hand, one-sided p-values for the MAR and the minimum $\\chi^2$.  Is this model for cosmology and errors consistent with the data in terms of these two statistics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Now, we are going to do some quick nested sampling.\n",
    "\n",
    "Install Nautilus with `pip install nautilus-sampler` or `conda install -c conda-forge nautilus-sampler`.  This program works in a similar way to `emcee`, which we used in tutorial 8b, but it does nested sampling instead of MCMC.\n",
    "The documentation is at https://nautilus-sampler.readthedocs.io/en/latest/index.html.\n",
    "\n",
    "Make a prior for the open model with uniform priors.\n",
    "\n",
    "Make a log-likelihood function for the data that takes the parameters only.  You can use the `mu_model_open()` function you already wrote.\n",
    "\n",
    "Run the sampler with `verbose=True`.  This may take a while.\n",
    "\n",
    "What is the evidence?  It is called `log_z` in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from nautilus import Prior\n",
    "from nautilus import Sampler\n",
    "\n",
    "prior = Prior()\n",
    "prior.add_parameter('M', dist=(20,50))\n",
    "prior.add_parameter(...)\n",
    "prior.add_parameter(...)\n",
    "\n",
    "def likelihood_open(params) :\n",
    "      .\n",
    "      .\n",
    "\n",
    "sampler = Sampler(prior, likelihood_open, n_live=1000)\n",
    "sampler.run(verbose=True)\n",
    "\n",
    "print('log of the evidence log Z: {:.2f}'.format(sampler.log_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Do the same as above but with the flat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_flat = Prior()\n",
    "prior_flat.add_parameter(...)\n",
    "prior_flat.add_parameter(...)\n",
    "\n",
    "def likelihood_flat(params) :\n",
    "    .\n",
    "    .\n",
    "\n",
    "sampler_flat = Sampler(prior_flat, likelihood_flat, n_live=1000)\n",
    "sampler_flat.run(verbose=True)\n",
    "\n",
    "print('log of the evidence log Z: {:.2f}'.format(sampler_flat.log_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Calculate Bayes' ratio for these models.  Do these data favor an open model or a flat one?  Is there strong evidence that the Universe is not flat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bayesian odds :\", ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) Make `corner` plots with the output from the output of Nautilus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import corner\n",
    "\n",
    "points, log_w, log_l = sampler.posterior()\n",
    "corner.corner(\n",
    "    points, weights=np.exp(log_w), bins=20, labels=prior.keys, color='blue',\n",
    "    plot_datapoints=False, range=np.repeat(0.999, len(prior.keys)))\n",
    "\n",
    "points, log_w, log_l = sampler_flat.posterior()\n",
    "corner.corner(\n",
    "    points, weights=np.exp(log_w), bins=20, labels=prior_flat.keys, color='blue',\n",
    "    plot_datapoints=False, range=np.repeat(0.999, len(prior_flat.keys)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
